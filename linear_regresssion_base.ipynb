{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linear_regresssion_base.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO04iHol2p/wkSuqzywEHcb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhangxs131/Machine_learning_notebook/blob/main/linear_regresssion_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#线性回归\n",
        "\n",
        "对于回归问题，最后类别往往是一个连续值进行选择，而非离散值，一般可能需要使用线性回归的方法，这里主要介绍简单线性回归，和局部加权线性回归。\n",
        "\n",
        "数据集为 鲍鱼年龄预测数据集：https://raw.githubusercontent.com/Jack-Cherish/Machine-Learning/master/Regression/abalone.txt"
      ],
      "metadata": {
        "id": "ra9iNU7hD_h7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSlZzu48D7sD",
        "outputId": "d151cc07-f606-4d96-e8e4-1f6d4f066610"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-05 02:51:21--  https://raw.githubusercontent.com/Jack-Cherish/Machine-Learning/master/Regression/abalone.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 193180 (189K) [text/plain]\n",
            "Saving to: ‘abalone.txt’\n",
            "\n",
            "\rabalone.txt           0%[                    ]       0  --.-KB/s               \rabalone.txt         100%[===================>] 188.65K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-05-05 02:51:21 (7.49 MB/s) - ‘abalone.txt’ saved [193180/193180]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#下载数据\n",
        "!wget https://raw.githubusercontent.com/Jack-Cherish/Machine-Learning/master/Regression/abalone.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#加载数据\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_data(filename):\n",
        "  with open(filename,'r') as f:\n",
        "    content=f.readlines()\n",
        "  \n",
        "\n",
        "  data,label=[],[]\n",
        "  for i in content:\n",
        "    t=i.strip().split('\\t')\n",
        "    data.append([float(j) for j in t[:-1]])\n",
        "    label.append(float(t[-1]))\n",
        "  \n",
        "  return data,label\n",
        "\n",
        "data,label=load_data('abalone.txt')"
      ],
      "metadata": {
        "id": "-e-V7kFjEgkC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#局部加权线性回归实现\n",
        "\n",
        "def lwlr(testPoint,data,label,k=1.0):\n",
        "  #k为高斯核的k值，k越小越拟合训练集\n",
        "\n",
        "  xMat=np.mat(data)\n",
        "  yMat=np.mat(label).T\n",
        "  m=xMat.shape[0]\n",
        "  weights=np.mat(np.eye((m))) #创建对焦矩阵\n",
        "\n",
        "  for j in range(m):\n",
        "    diffMat=testPoint-xMat[j,:]\n",
        "    weights[j,j]=np.exp(diffMat*diffMat.T/(-2.0*k**2))\n",
        "  \n",
        "  xTx=xMat.T*(weights*xMat)\n",
        "  if np.linalg.det(xTx)==0.0:\n",
        "    return \n",
        "  ws=xTx.I *(xMat.T*(weights*yMat))\n",
        "\n",
        "  return testPoint*ws\n",
        "\n",
        "def lwlrTest(testArr,data,label,k=1.0):\n",
        "  testArr=np.mat(testArr)\n",
        "  m=testArr.shape[0]\n",
        "  yHat=np.zeros(m)\n",
        "  for i in range(m):\n",
        "    yHat[i]=lwlr(testArr[i],data,label,k)\n",
        "  return yHat\n",
        "\n",
        "def standRegres(xArr,yArr):\n",
        "    xMat = np.mat(xArr); yMat = np.mat(yArr).T\n",
        "    xTx = xMat.T * xMat                            #根据文中推导的公示计算回归系数\n",
        "    if np.linalg.det(xTx) == 0.0:\n",
        "        print(\"矩阵为奇异矩阵,不能求逆\")\n",
        "        return\n",
        "    ws = xTx.I * (xMat.T*yMat)\n",
        "    return ws\n",
        "\n",
        "  \n",
        "def rssError(yArr,yHatArr):\n",
        "  return ((yArr-yHatArr)**2).sum()\n",
        "\n",
        "abX,abY=data,label\n",
        "print('训练集与测试集相同:局部加权线性回归,核k的大小对预测的影响:')\n",
        "yHat01 = lwlrTest(abX[0:99], abX[0:99], abY[0:99], 0.1)\n",
        "yHat1 = lwlrTest(abX[0:99], abX[0:99], abY[0:99], 1)\n",
        "yHat10 = lwlrTest(abX[0:99], abX[0:99], abY[0:99], 10)\n",
        "print('k=0.1时,误差大小为:',rssError(abY[0:99], yHat01.T))\n",
        "print('k=1  时,误差大小为:',rssError(abY[0:99], yHat1.T))\n",
        "print('k=10 时,误差大小为:',rssError(abY[0:99], yHat10.T))\n",
        "print('')\n",
        "print('训练集与测试集不同:局部加权线性回归,核k的大小是越小越好吗？更换数据集,测试结果如下:')\n",
        "yHat01 = lwlrTest(abX[100:199], abX[0:99], abY[0:99], 0.1)\n",
        "yHat1 = lwlrTest(abX[100:199], abX[0:99], abY[0:99], 1)\n",
        "yHat10 = lwlrTest(abX[100:199], abX[0:99], abY[0:99], 10)\n",
        "print('k=0.1时,误差大小为:',rssError(abY[100:199], yHat01.T))\n",
        "print('k=1  时,误差大小为:',rssError(abY[100:199], yHat1.T))\n",
        "print('k=10 时,误差大小为:',rssError(abY[100:199], yHat10.T))\n",
        "print('')\n",
        "print('训练集与测试集不同:简单的线性归回与k=1时的局部加权线性回归对比:')\n",
        "print('k=1时,误差大小为:', rssError(abY[100:199], yHat1.T))\n",
        "ws = standRegres(abX[0:99], abY[0:99])\n",
        "yHat = np.mat(abX[100:199]) * ws\n",
        "print('简单的线性回归误差大小:', rssError(abY[100:199], yHat.T.A))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDUaWXQ7FnXC",
        "outputId": "211ef2ec-53d1-49d4-8b86-096c7183a9bf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "训练集与测试集相同:局部加权线性回归,核k的大小对预测的影响:\n",
            "k=0.1时,误差大小为: 56.78868743048742\n",
            "k=1  时,误差大小为: 429.8905618704059\n",
            "k=10 时,误差大小为: 549.1181708828803\n",
            "\n",
            "训练集与测试集不同:局部加权线性回归,核k的大小是越小越好吗？更换数据集,测试结果如下:\n",
            "k=0.1时,误差大小为: 57913.51550155909\n",
            "k=1  时,误差大小为: 573.5261441894984\n",
            "k=10 时,误差大小为: 517.5711905381573\n",
            "\n",
            "训练集与测试集不同:简单的线性归回与k=1时的局部加权线性回归对比:\n",
            "k=1时,误差大小为: 573.5261441894984\n",
            "简单的线性回归误差大小: 518.6363153245542\n"
          ]
        }
      ]
    }
  ]
}